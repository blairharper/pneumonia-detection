{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workbook proposes a solution to the Kaggle challenge on pneumonia detection posted by RSNA. It draws on Zahaviguy's 'What are lunch opacities' kernel (https://www.kaggle.com/zahaviguy/what-are-lung-opacities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pydicom, numpy as np\n",
    "from sklearn.datasets import load_files \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.models import Model, Input, Sequential\n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "# Set paths\n",
    "\n",
    "S1_TRAIN_IMGS = \"./img/train_png/\"\n",
    "S1_VALID_IMGS = \"./img/valid_png/\"\n",
    "S1_TEST_IMGS = \"./img/test_png/\"\n",
    "S1_LABELS = \"./img/labels/stage_1_train_labels.csv\"\n",
    "S1_CLASS_INFO = \"./stage_1_detailed_class_info.csv\"\n",
    "\n",
    "# Helper to parse CSV file\n",
    "\n",
    "def parse_csv(df):\n",
    "    extract_box = lambda row: [row['y'], row['x'], row['height'], row['width']]\n",
    "    parsed = {}\n",
    "    \n",
    "    for n, row in df.iterrows():\n",
    "        # --- Initialize patient entry into parsed \n",
    "        pid = row['patientId']\n",
    "        if pid not in parsed:\n",
    "            parsed[pid] = {\n",
    "                'dicom': S1_TRAIN_IMGS + '{0}.dcm.png'.format(pid),\n",
    "                'label': row['Target'],\n",
    "                'boxes': []\n",
    "            }\n",
    "        if parsed[pid]['label'] == 1:\n",
    "            parsed[pid]['boxes'].append(extract_box(row))\n",
    "    \n",
    "    return parsed\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17709702840872271409\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10670292992\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2985982445329014845\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['patientId', 'x', 'y', 'width', 'height', 'Target']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(S1_LABELS)\n",
    "\n",
    "patient_class = pd.read_csv(S1_CLASS_INFO, index_col=0)\n",
    "\n",
    "parsed = parse_csv(df)\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dicom': './img/train_png/0004cfab-14fd-4e49-80ba-63a80b6bddd6.dcm.png', 'label': 0, 'boxes': []}\n",
      "class    No Lung Opacity / Not Normal\n",
      "Name: 0004cfab-14fd-4e49-80ba-63a80b6bddd6, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Explore some data using our parser\n",
    "\n",
    "patient_0 = df['patientId'][0]\n",
    "print(parsed[patient_0])\n",
    "print(patient_class.loc[patient_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(data):\n",
    "    \"\"\"\n",
    "    Draw single patient with bounding boxes\n",
    "    \"\"\"\n",
    "    \n",
    "    di = pydicom.read_file(data['dicom'])\n",
    "    img = di.pixel_array\n",
    "    \n",
    "    img = np.stack([img] * 3, axis = 2)\n",
    "    \n",
    "    # add boxes with random colour if present\n",
    "    for box in data['boxes']:\n",
    "        rgb = np.floor(np.random.rand(3) * 256).astype('int')\n",
    "        img = overlay_box(img=img, box=box, rgb=rgb, stroke=6)\n",
    "        \n",
    "    plt.imshow(img, cmap=plt.cm.gist_gray)\n",
    "    plt.axis('off')\n",
    "    \n",
    "def overlay_box(img, box, rgb, stroke=1):\n",
    "    box = [int(b) for b in box]\n",
    "    \n",
    "    y1, x1, height, width = box\n",
    "    y2 = y1 + height\n",
    "    x2 = x1 + width\n",
    "    \n",
    "    img[y1:y1 + stroke, x1:x2] = rgb\n",
    "    img[y2:y2 + stroke, x1:x2] = rgb\n",
    "    img[y1:y2, x1:x1 + stroke] = rgb\n",
    "    img[y1:y2, x2:x2 + stroke] = rgb\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class    Normal\n",
      "Name: 003d8fa0-6bf1-40ed-b54c-ac657f8495c5, dtype: object\n"
     ]
    }
   ],
   "source": [
    "patient_test = df['patientId'][3]\n",
    "print(patient_class.loc[patient_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    \"\"\"takes an image path and returns a 4D array/tensor \n",
    "    for use with Keras CNN using TensorFlow backend:\n",
    "    (nb_samples, 224, 224, 3)\"\"\"\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(df):\n",
    "    list_of_tensors =[path_to_tensor(parsed[pid]['dicom']) for pid in tqdm(df['patientId'])]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "def target_to_tensor(df):\n",
    "    list_of_tensors =[parsed[pid]['label'] for pid in df['patientId']]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24848/24848 [02:29<00:00, 166.35it/s]\n",
      "100%|██████████| 4141/4141 [00:24<00:00, 167.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "train_tensors = paths_to_tensor(df[:24848])\n",
    "valid_tensors = paths_to_tensor(df[24848:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = target_to_tensor(df[:24848])\n",
    "valid_targets = target_to_tensor(df[24848:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b/anaconda3/envs/py3/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "Resnet50_model = ResNet50(weights='imagenet', include_top=False, input_shape=train_tensors[0].shape)\n",
    "for layer in Resnet50_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rotation_range=45,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.25,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Resnet50_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "\n",
    "r50_transfer = Model(inputs=Resnet50_model.input, outputs=predictions)\n",
    "r50_transfer.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(train_targets)\n",
    "y_valid = to_categorical(valid_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='weights_best_Resnet50-structure-v0.4.hdf5',\n",
    "                              verbose=1, save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24848 samples, validate on 4141 samples\n",
      "Epoch 1/10\n",
      "24848/24848 [==============================] - 81s 3ms/step - loss: 0.6107 - acc: 0.6887 - val_loss: 0.3747 - val_acc: 0.8104\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37466, saving model to weights_best_Resnet50-structure-v0.4.hdf5\n",
      "Epoch 2/10\n",
      "24848/24848 [==============================] - 87s 3ms/step - loss: 0.5320 - acc: 0.7175 - val_loss: 0.4672 - val_acc: 0.8124\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.37466\n",
      "Epoch 3/10\n",
      "24832/24848 [============================>.] - ETA: 0s - loss: 0.5000 - acc: 0.7308"
     ]
    }
   ],
   "source": [
    "r50_transfer.fit(train_tensors, y_train, epochs=10,\n",
    "                batch_size=batch_size, callbacks=[checkpointer],\n",
    "                validation_data=(valid_tensors, y_valid), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model version 0.2 - Observations & Next steps\n",
    "> x = Resnet50_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    ">r50_transfer = Model(inputs=Resnet50_model.input, outputs=predictions)\n",
    "r50_transfer.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    ">batch_size = 32\n",
    "\n",
    "**Observations**:\n",
    "A _validation accuracy_ of 0.8190 was achieved by epoch 3 with a _loss_ of 2.89894. This did not increase again until epoch 8, which achieved a _validation accuracy_ of 0.8239 with a _loss_ of 2.83392.\n",
    "\n",
    "**Next steps**:\n",
    "Increase training sample size and reduce validation sample size, using more of the data for training. Previous training sample size was 22000. I have increased this to: 24848.\n",
    "\n",
    "Old training sample size: 22000\n",
    "Old validation sample size: 6989\n",
    "\n",
    "New training sample size: 24848\n",
    "New validation sample size 6989\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model version 0.3 - Observations & Next steps\n",
    ">x = Resnet50_model.output\n",
    ">x = Flatten()(x)\n",
    ">x = Dense(128, activation='relu')(x)\n",
    ">x = Dropout(0.5)(x)\n",
    ">x = Dense(128, activation='relu')(x)\n",
    ">x = Dropout(0.5)(x)\n",
    ">predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    ">r50_transfer = Model(inputs=Resnet50_model.input, outputs=predictions)\n",
    ">r50_transfer.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    ">batch_size = 20\n",
    "\n",
    "**Changes made**:\n",
    "I have made the changes to the sample sizes as per findings in version 0.2. I have also decreased the batch size from 32 to 20, based on the paper \"On Large-Batch Training for Deep Learning\" which found that larger batches degraded the quality of the model.\n",
    "\n",
    "**Observations**:\n",
    "These changes resulted in the model converging very quickly, to an accuracy of 0.8013 with a validation loss of 3.20338. The model then oscillated and results did not improve for the remaining 9 epochs.\n",
    "\n",
    "**Next steps**:\n",
    "Return batch size to 32 to determine the cause of the effect (changes to training sample sizes or batch size). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model version 0.4 - Observations & Next steps\n",
    ">x = Resnet50_model.output\n",
    ">x = Flatten()(x)\n",
    ">x = Dense(128, activation='relu')(x)\n",
    ">x = Dropout(0.5)(x)\n",
    ">x = Dense(128, activation='relu')(x)\n",
    ">x = Dropout(0.5)(x)\n",
    ">predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    ">r50_transfer = Model(inputs=Resnet50_model.input, outputs=predictions)\n",
    ">r50_transfer.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    ">batch_size = 32\n",
    "\n",
    "**Changes made**:\n",
    "Returned batch size to 32 after poor results from 0.3. It seemed unreasonable to assume that providing the model with more training data would have the effects noted in 0.3.\n",
    "\n",
    "**Observations**:\n",
    "Similar observations made to 0.2.\n",
    "\n",
    "**Next steps**:\n",
    "Reduce learning rate of adam optimizer from default of 0.001 to 1e-4. Perhaps this will help the model identified more nuanced features associated with lung opacities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model version 0.5 - Observations & Next steps\n",
    ">x = Resnet50_model.output\n",
    ">x = Flatten()(x)\n",
    ">x = Dense(128, activation='relu')(x)\n",
    ">x = Dropout(0.5)(x)\n",
    ">x = Dense(128, activation='relu')(x)\n",
    ">x = Dropout(0.5)(x)\n",
    ">predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    ">r50_transfer = Model(inputs=Resnet50_model.input, outputs=predictions)\n",
    ">r50_transfer.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    ">batch_size = 32\n",
    "\n",
    "**Changes made**:\n",
    "Reduce learning rate of adam optimizer from default of 0.001 to 1e-4. \n",
    "\n",
    "**Observations**:\n",
    "Significant reduction in loss from a best of 2.898 in v0.2 down to 0.50808. \n",
    "Accuracy also increased to 0.83 (previous high of 0.82 in v0.2). However, there is also evidence of overfitting around the 4th epoch onwards.\n",
    "\n",
    "**Next steps**:\n",
    "Reduce network size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model version 0.5 - Observations & Next steps\n",
    ">x = Resnet50_model.output\n",
    ">x = Flatten()(x)\n",
    ">x = Dense(128, activation='relu')(x)\n",
    ">x = Dropout(0.5)(x)\n",
    ">x = Dense(64, activation='relu')(x)\n",
    ">x = Dropout(0.5)(x)\n",
    ">predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    ">r50_transfer = Model(inputs=Resnet50_model.input, outputs=predictions)\n",
    ">r50_transfer.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    ">batch_size = 32\n",
    "\n",
    "**Changes made**:\n",
    "Reduce learning rate of adam optimizer from default of 0.001 to 1e-4. \n",
    "\n",
    "**Observations**:\n",
    "Significant reduction in loss from a best of 2.898 in v0.2 down to 0.50808. \n",
    "Accuracy also increased to 0.83 (previous high of 0.82 in v0.2). However, there is also evidence of overfitting around the 4th epoch onwards.\n",
    "\n",
    "**Next steps**:\n",
    "Reduce network size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
