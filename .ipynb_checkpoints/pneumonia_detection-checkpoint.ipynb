{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect pneuomnia from x-ray images of lungs\n",
    "\n",
    "This project analyses x-ray images of lungs to identify pneumonia. It takes a hollistic approach and does not segment the image to identify different areas of lunch opacity. Nor does it use any data contained within the DICOM image format (such as age of patient / sex / scan angle) - the images have been pre-converted to png files outside of this notebook to allow for normal keras preprocessing.\n",
    "\n",
    "The model applies resnet 50 transfer learning.\n",
    " \n",
    "A number of fine tuning adjustments were made over several iterations which can be observed at the end of this file.\n",
    "\n",
    "The data used is from the Kaggle dataset found at: https://www.kaggle.com/c/rsna-pneumonia-detection-challenge but note that this notebook does not present a complete solution to this competition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pydicom, numpy as np\n",
    "from sklearn.datasets import load_files \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.models import Model, Input, Sequential\n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "# Set paths  - needs updating, we don't ever use S1_VALID_IMGS\n",
    "S1_TRAIN_IMGS = \"./img/train_png/\"\n",
    "S1_VALID_IMGS = \"./img/valid_png/\"\n",
    "S1_TEST_IMGS = \"./img/test_png/\"\n",
    "S1_LABELS = \"./img/labels/stage_1_train_labels.csv\"\n",
    "S1_CLASS_INFO = \"./stage_1_detailed_class_info.csv\"\n",
    "\n",
    "# Helper to parse CSV file into dictionary\n",
    "\n",
    "def parse_csv(df):\n",
    "    extract_box = lambda row: [row['y'], row['x'], row['height'], row['width']]\n",
    "    parsed = {}\n",
    "    \n",
    "    for n, row in df.iterrows():\n",
    "        # --- Initialize patient entry into parsed \n",
    "        pid = row['patientId']\n",
    "        if pid not in parsed:\n",
    "            parsed[pid] = {\n",
    "                'dicom': S1_TRAIN_IMGS + '{0}.dcm.png'.format(pid),\n",
    "                'label': row['Target'],\n",
    "                'boxes': []\n",
    "            }\n",
    "        if parsed[pid]['label'] == 1:\n",
    "            parsed[pid]['boxes'].append(extract_box(row))\n",
    "    \n",
    "    return parsed            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17943198189765818006\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10654977229\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 11907391551141067955\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Check which device(s) are detected by tensorflow backend to ensure GPU is detected\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(S1_LABELS)\n",
    "\n",
    "patient_class = pd.read_csv(S1_CLASS_INFO, index_col=0)\n",
    "\n",
    "parsed = parse_csv(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dicom': './img/train_png/0004cfab-14fd-4e49-80ba-63a80b6bddd6.dcm.png', 'label': 0, 'boxes': []}\n",
      "class    No Lung Opacity / Not Normal\n",
      "Name: 0004cfab-14fd-4e49-80ba-63a80b6bddd6, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Explore some data using our parser\n",
    "patient_0 = df['patientId'][0]\n",
    "print(parsed[patient_0])\n",
    "print(patient_class.loc[patient_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(data):\n",
    "    \"\"\"\n",
    "    Draw single patient with bounding boxes\n",
    "    \"\"\"\n",
    "    \n",
    "    di = pydicom.read_file(data['dicom'])\n",
    "    img = di.pixel_array\n",
    "    \n",
    "    img = np.stack([img] * 3, axis = 2)\n",
    "    \n",
    "    # add boxes with random colour if present\n",
    "    for box in data['boxes']:\n",
    "        rgb = np.floor(np.random.rand(3) * 256).astype('int')\n",
    "        img = overlay_box(img=img, box=box, rgb=rgb, stroke=6)\n",
    "        \n",
    "    plt.imshow(img, cmap=plt.cm.gist_gray)\n",
    "    plt.axis('off')\n",
    "    \n",
    "def overlay_box(img, box, rgb, stroke=1):\n",
    "    box = [int(b) for b in box]\n",
    "    \n",
    "    y1, x1, height, width = box\n",
    "    y2 = y1 + height\n",
    "    x2 = x1 + width\n",
    "    \n",
    "    img[y1:y1 + stroke, x1:x2] = rgb\n",
    "    img[y2:y2 + stroke, x1:x2] = rgb\n",
    "    img[y1:y2, x1:x1 + stroke] = rgb\n",
    "    img[y1:y2, x2:x2 + stroke] = rgb\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class    Normal\n",
      "Name: 003d8fa0-6bf1-40ed-b54c-ac657f8495c5, dtype: object\n"
     ]
    }
   ],
   "source": [
    "patient_test = df['patientId'][3]\n",
    "print(patient_class.loc[patient_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    \"\"\"takes an image path and returns a 4D array/tensor \n",
    "    for use with Keras CNN using TensorFlow backend:\n",
    "    (nb_samples, 224, 224, 3)\"\"\"\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(df):\n",
    "    list_of_tensors =[path_to_tensor(parsed[pid]['dicom']) for pid in tqdm(df['patientId'])]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "def target_to_tensor(df):\n",
    "    list_of_tensors =[parsed[pid]['label'] for pid in df['patientId']]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24848/24848 [02:29<00:00, 165.65it/s]\n",
      "100%|██████████| 4141/4141 [00:25<00:00, 160.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "train_tensors = paths_to_tensor(df[:24848])\n",
    "valid_tensors = paths_to_tensor(df[24848:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = target_to_tensor(df[:24848])\n",
    "valid_targets = target_to_tensor(df[24848:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b/anaconda3/envs/py3/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "# Set up resnet50 model and freeze the top layers - we don't want to retrain them\n",
    "Resnet50_model = ResNet50(weights='imagenet', include_top=False, input_shape=train_tensors[0].shape)\n",
    "for layer in Resnet50_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image augmentation to improve accuracy and reduce overfitting\n",
    "train_datagen = ImageDataGenerator(rotation_range=45,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.25,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some dense layers and a final output layer (true or false for pneumonia detection)\n",
    "x = Resnet50_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "\n",
    "# Tuning parameters\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "\n",
    "r50_transfer = Model(inputs=Resnet50_model.input, outputs=predictions)\n",
    "r50_transfer.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model was not training with binary cross entropy despite only being 2 classes.\n",
    "# Switched to categorical cross entropy and so need to convert labels to categorical.\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(train_targets)\n",
    "y_valid = to_categorical(valid_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint to save the model when new best val_loss is achieved\n",
    "checkpointer = ModelCheckpoint(filepath='weights_best_Resnet50-structure-v0.4.hdf5',\n",
    "                              verbose=1, save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24848 samples, validate on 4141 samples\n",
      "Epoch 1/10\n",
      "24848/24848 [==============================] - 83s 3ms/step - loss: 0.6112 - acc: 0.6812 - val_loss: 0.4419 - val_acc: 0.8013\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.44187, saving model to weights_best_Resnet50-structure-v0.4.hdf5\n",
      "Epoch 2/10\n",
      "24848/24848 [==============================] - 80s 3ms/step - loss: 0.5353 - acc: 0.7039 - val_loss: 0.5624 - val_acc: 0.8022\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.44187\n",
      "Epoch 3/10\n",
      "24832/24848 [============================>.] - ETA: 0s - loss: 0.5052 - acc: 0.7153"
     ]
    }
   ],
   "source": [
    "r50_transfer.fit(train_tensors, y_train, epochs=10,\n",
    "                batch_size=batch_size, callbacks=[checkpointer],\n",
    "                validation_data=(valid_tensors, y_valid), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model version 0.2 - Observations & Next steps\n",
    "> x = Resnet50_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    ">r50_transfer = Model(inputs=Resnet50_model.input, outputs=predictions)\n",
    "r50_transfer.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    ">batch_size = 32\n",
    "\n",
    "**Observations**:\n",
    "A _validation accuracy_ of 0.8190 was achieved by epoch 3 with a _loss_ of 2.89894. This did not increase again until epoch 8, which achieved a _validation accuracy_ of 0.8239 with a _loss_ of 2.83392.\n",
    "\n",
    "**Next steps**:\n",
    "Increase training sample size and reduce validation sample size, using more of the data for training. Previous training sample size was 22000. I have increased this to: 24848.\n",
    "\n",
    "Old training sample size: 22000\n",
    "Old validation sample size: 6989\n",
    "\n",
    "New training sample size: 24848\n",
    "New validation sample size 6989\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model version 0.3 - Observations & Next steps\n",
    ">x = Resnet50_model.output\n",
    ">x = Flatten()(x)\n",
    ">x = Dense(128, activation='relu')(x)\n",
    ">x = Dropout(0.5)(x)\n",
    ">x = Dense(128, activation='relu')(x)\n",
    ">x = Dropout(0.5)(x)\n",
    ">predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    ">r50_transfer = Model(inputs=Resnet50_model.input, outputs=predictions)\n",
    ">r50_transfer.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    ">batch_size = 20\n",
    "\n",
    "**Changes made**:\n",
    "I have made the changes to the sample sizes as per findings in version 0.2. I have also decreased the batch size from 32 to 20, based on the paper \"On Large-Batch Training for Deep Learning\" which found that larger batches degraded the quality of the model.\n",
    "\n",
    "**Observations**:\n",
    "These changes resulted in the model converging very quickly, to an accuracy of 0.8013 with a validation loss of 3.20338. The model then oscillated and results did not improve for the remaining 9 epochs.\n",
    "\n",
    "**Next steps**:\n",
    "Return batch size to 32 to determine the cause of the effect (changes to training sample sizes or batch size). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model version 0.4 - Observations & Next steps\n",
    ">x = Resnet50_model.output\n",
    ">x = Flatten()(x)\n",
    ">x = Dense(128, activation='relu')(x)\n",
    ">x = Dropout(0.5)(x)\n",
    ">x = Dense(128, activation='relu')(x)\n",
    ">x = Dropout(0.5)(x)\n",
    ">predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    ">r50_transfer = Model(inputs=Resnet50_model.input, outputs=predictions)\n",
    ">r50_transfer.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    ">batch_size = 32\n",
    "\n",
    "**Changes made**:\n",
    "Returned batch size to 32 after poor results from 0.3. It seemed unreasonable to assume that providing the model with more training data would have the effects noted in 0.3.\n",
    "\n",
    "**Observations**:\n",
    "Similar observations made to 0.2.\n",
    "\n",
    "**Next steps**:\n",
    "Reduce learning rate of adam optimizer from default of 0.001 to 1e-4. Perhaps this will help the model identified more nuanced features associated with lung opacities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model version 0.5 - Observations & Next steps\n",
    ">x = Resnet50_model.output\n",
    ">x = Flatten()(x)\n",
    ">x = Dense(128, activation='relu')(x)\n",
    ">x = Dropout(0.5)(x)\n",
    ">x = Dense(128, activation='relu')(x)\n",
    ">x = Dropout(0.5)(x)\n",
    ">predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    ">r50_transfer = Model(inputs=Resnet50_model.input, outputs=predictions)\n",
    ">r50_transfer.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    ">batch_size = 32\n",
    "\n",
    "**Changes made**:\n",
    "Reduce learning rate of adam optimizer from default of 0.001 to 1e-4. \n",
    "\n",
    "**Observations**:\n",
    "Significant reduction in loss from a best of 2.898 in v0.2 down to 0.50808. \n",
    "Accuracy also increased to 0.83 (previous high of 0.82 in v0.2). However, there is also evidence of overfitting around the 4th epoch onwards.\n",
    "\n",
    "**Next steps**:\n",
    "Reduce network size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model version 0.5 - Observations & Next steps\n",
    ">x = Resnet50_model.output\n",
    ">x = Flatten()(x)\n",
    ">x = Dense(128, activation='relu')(x)\n",
    ">x = Dropout(0.5)(x)\n",
    ">x = Dense(64, activation='relu')(x)\n",
    ">x = Dropout(0.5)(x)\n",
    ">predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    ">r50_transfer = Model(inputs=Resnet50_model.input, outputs=predictions)\n",
    ">r50_transfer.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    ">batch_size = 32\n",
    "\n",
    "**Changes made**:\n",
    "Reduce learning rate of adam optimizer from default of 0.001 to 1e-4. \n",
    "\n",
    "**Observations**:\n",
    "Significant reduction in loss from a best of 2.898 in v0.2 down to 0.50808. \n",
    "Accuracy also increased to 0.83 (previous high of 0.82 in v0.2). However, there is also evidence of overfitting around the 4th epoch onwards.\n",
    "\n",
    "**Next steps**:\n",
    "Reduce network size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
